{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing checkpoints manually\n",
    "\n",
    "The persistent state of a TensorFlow model is stored in `tf.Variable` objects. These can be constructed directly, but are often created through high-level APIs like `tf.keras.layers`.\n",
    "\n",
    "The easiest way to manage variables is by attaching them to Python objects, then referencing those objects. Subclasses of `tf.train.Checkpoint`, `tf.keras.layers.Layer`, and `tf.keras.Model` automatically track variables assigned to their attributes. The following example constructs a simple linear model, then writes checkpoints which contain values for all of the model's variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(tf.keras.Model):\n",
    "    \"\"\"A simple linear model.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = tf.keras.layers.Dense(5)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.l1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's not the focus of this guide, to be executable the example needs data and an optimization step. The model will train on slices of an in-memory dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_dataset():\n",
    "    inputs = tf.range(10.)[:, None]\n",
    "    labels = inputs * 5. + tf.range(5.)[None, :]\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        dict(x=inputs, y=labels)).repeat(10).batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, example, optimizer):\n",
    "    \"\"\"Trains `net` on `example` using `optimizer`.\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = net(example['x'])\n",
    "        loss = tf.reduce_mean(tf.abs(output - example['y']))\n",
    "    variables = net.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following training loop creates an instance of the model and of an optimizer, then gathers them into a `tf.train.Checkpoint` object. It calls the training step in a loop on each batch of data, and periodically writes checkpoints to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n",
      "Saved checkpoint for step 10: ./tf_ckpts\\ckpt-1\n",
      "loss 26.75\n",
      "Saved checkpoint for step 20: ./tf_ckpts\\ckpt-2\n",
      "loss 20.17\n",
      "Saved checkpoint for step 30: ./tf_ckpts\\ckpt-3\n",
      "loss 13.61\n",
      "Saved checkpoint for step 40: ./tf_ckpts\\ckpt-4\n",
      "loss 7.15\n",
      "Saved checkpoint for step 50: ./tf_ckpts\\ckpt-5\n",
      "loss 2.11\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.1)\n",
    "net = Net()\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net)\n",
    "manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "    \n",
    "for example in toy_dataset():\n",
    "    loss = train_step(net, example, opt)\n",
    "    ckpt.step.assign_add(1)\n",
    "    if int(ckpt.step) % 10 == 0:\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "        print(\"loss {:1.2f}\".format(loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding snippet will randomly initialize the model variables when it first runs. After the first run it will resume training from where it left off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./tf_ckpts\\ckpt-5\n",
      "Saved checkpoint for step 60: ./tf_ckpts\\ckpt-6\n",
      "loss 1.16\n",
      "Saved checkpoint for step 70: ./tf_ckpts\\ckpt-7\n",
      "loss 1.23\n",
      "Saved checkpoint for step 80: ./tf_ckpts\\ckpt-8\n",
      "loss 0.69\n",
      "Saved checkpoint for step 90: ./tf_ckpts\\ckpt-9\n",
      "loss 0.59\n",
      "Saved checkpoint for step 100: ./tf_ckpts\\ckpt-10\n",
      "loss 0.32\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(0.1)\n",
    "net = Net()\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net)\n",
    "manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "\n",
    "for example in toy_dataset():\n",
    "    loss = train_step(net, example, opt)\n",
    "    ckpt.step.assign_add(1)\n",
    "    if int(ckpt.step) % 10 == 0:\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "        print(\"loss {:1.2f}\".format(loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.train.CheckpointManager` object deletes old checkpoints. Above it's configured to keep only the three most recent checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./tf_ckpts\\\\ckpt-8', './tf_ckpts\\\\ckpt-9', './tf_ckpts\\\\ckpt-10']\n"
     ]
    }
   ],
   "source": [
    "print(manager.checkpoints) # list the three remaining checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These paths, e.g. `'./tf_ckpts/ckpt-10'`, are not files on disk. Instead they are prefixes for an `index` file and one or more data files which contain the variable values. These prefixes are grouped together in a single `checkpoint` file (`'./tf_ckpts/checkpoint'`) where the `CheckpointManager` saves its state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint',\n",
       " 'ckpt-10.data-00000-of-00002',\n",
       " 'ckpt-10.data-00001-of-00002',\n",
       " 'ckpt-10.index',\n",
       " 'ckpt-8.data-00000-of-00002',\n",
       " 'ckpt-8.data-00001-of-00002',\n",
       " 'ckpt-8.index',\n",
       " 'ckpt-9.data-00000-of-00002',\n",
       " 'ckpt-9.data-00001-of-00002',\n",
       " 'ckpt-9.index']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"./tf_ckpts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"loading_mechanics\"/>\n",
    "## Loading mechanics\n",
    "\n",
    "TensorFlow matches variables to checkpointed values by traversing a directed graph with named edges, starting from the object being loaded. Edge names typically come from attribute names in objects, for example the `\"l1\"` in `self.l1 = tf.keras.layers.Dense(5)`. `tf.train.Checkpoint` uses its keyword argument names, as in the `\"step\"` in `tf.train.Checkpoint(step=...)`.\n",
    "\n",
    "The dependency graph from the example above looks like this:\n",
    "\n",
    "![Visualization of the dependency graph for the example training loop](http://tensorflow.org/images/guide/whole_checkpoint.svg)\n",
    "\n",
    "With the optimizer in red, regular variables in blue, and optimizer slot variables in orange. The other nodes, for example representing the `tf.train.Checkpoint`, are black.\n",
    "\n",
    "Slot variables are part of the optimizer's state, but are created for a specific variable. For example the `'m'` edges above correspond to momentum, which the Adam optimizer tracks for each variable. Slot variables are only saved in a checkpoint if the variable and the optimizer would both be saved, thus the dashed edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `restore()` on a `tf.train.Checkpoint` object queues the requested restorations, restoring variable values as soon as there's a matching path from the `Checkpoint` object. For example we can load just the kernel from the model we defined above by reconstructing one path to it through the network and the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[3.0906975 2.115607  2.7918575 2.8857708 4.059075 ]\n"
     ]
    }
   ],
   "source": [
    "to_restore = tf.Variable(tf.zeros([5]))\n",
    "print(to_restore.numpy()) # all zeros\n",
    "fake_layer = tf.train.Checkpoint(bias=to_restore)\n",
    "fake_net = tf.train.Checkpoint(l1=fake_layer)\n",
    "new_root = tf.train.Checkpoint(net=fake_net)\n",
    "status = new_root.restore(tf.train.latest_checkpoint('./tf_ckpts/'))\n",
    "print(to_restore.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependency graph for these new objects is a much smaller subgraph of the larger checkpoint we wrote above. It includes only the bias and a save counter that `tf.train.Checkpoint` uses to number checkpoints.\n",
    "\n",
    "![Visualization of a subgraph for the bias variable](http://tensorflow.org/images/guide/partial_checkpoint.svg)\n",
    "\n",
    "`restore()` returns a status object, which has optional assertions. All of the objects we've created in our new `Checkpoint` have been restored, so `status.assert_existing_objects_matched()` passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1beab4439b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many objects in the checkpoint which haven't matched, including the layer's kernel and the optimizer's variables. `status.assert_consumed()` only passes if the checkpoint and the program match exactly, and would throw an exception here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed restorations\n",
    "\n",
    "`Layer` objects in TensorFlow may delay the creation of variables to their first call, when input shapes are available. For example the shape of a `Dense` layer's kernel depends on both the layer's input and output shapes, and so the output shape required as a constructor argument is not enough information to create the variable on its own. Since calling a `Layer` also reads the variable's value, a restore must happen between the variable's creation and its first use.\n",
    "\n",
    "To support this idiom, `tf.train.Checkpoint` queues restores which don't yet have a matching variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]]\n",
      "[[4.5674243 4.8244634 4.8828235 5.0211086 4.982023 ]]\n"
     ]
    }
   ],
   "source": [
    "delayed_restore = tf.Variable(tf.zeros([1, 5]))\n",
    "print(delayed_restore.numpy())\n",
    "fake_layer.kernel = delayed_restore\n",
    "print(delayed_restore.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually inspecting checkpoints\n",
    "\n",
    "`tf.train.list_variables` lists the checkpoint keys and shapes of variables in a checkpoint. Checkpoint keys are paths in the graph displayed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\n",
       " ('net/l1/bias/.ATTRIBUTES/VARIABLE_VALUE', [5]),\n",
       " ('net/l1/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE', [5]),\n",
       " ('net/l1/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE', [5]),\n",
       " ('net/l1/kernel/.ATTRIBUTES/VARIABLE_VALUE', [1, 5]),\n",
       " ('net/l1/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE',\n",
       "  [1, 5]),\n",
       " ('net/l1/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE',\n",
       "  [1, 5]),\n",
       " ('optimizer/beta_1/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
       " ('optimizer/beta_2/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
       " ('optimizer/decay/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
       " ('optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
       " ('optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
       " ('save_counter/.ATTRIBUTES/VARIABLE_VALUE', []),\n",
       " ('step/.ATTRIBUTES/VARIABLE_VALUE', [])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.list_variables(tf.train.latest_checkpoint('./tf_ckpts/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List and dictionary tracking\n",
    "\n",
    "As with direct attribute assignments like `self.l1 = tf.keras.layers.Dense(5)`, assigning lists and dictionaries to attributes will track their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = tf.train.Checkpoint()\n",
    "save.listed = [tf.Variable(1.)]\n",
    "save.listed.append(tf.Variable(2.))\n",
    "save.mapped = {'one': save.listed[0]}\n",
    "save.mapped['two'] = save.listed[1]\n",
    "save_path = save.save('./tf_list_example')\n",
    "\n",
    "restore = tf.train.Checkpoint()\n",
    "v2 = tf.Variable(0.)\n",
    "assert 0. == v2.numpy() # not restored yet\n",
    "restore.mapped = {'two': v2}\n",
    "restore.restore(save_path)\n",
    "assert 2. == v2.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice wrapper objects for lists and dictionaries. These wrappers are checkpointable versions of the underlying data-structures. Just like the attribute based loading, these wrappers restore a variable's value as soon as it's added to the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper([])\n"
     ]
    }
   ],
   "source": [
    "restore.listed = []\n",
    "print(restore.listed) # ListWrapper([])\n",
    "v1 = tf.Variable(0.)\n",
    "restore.listed.append(v1)\n",
    "assert 1. == v1.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same tracking is automatically applied to subclasses of `tf.keras.Model`, and may be used for example to track lists of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
